{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Syclops is a tool for creating synthetic data from 3D virtual environments.</p>"},{"location":"#features","title":"\ud83c\udfaf Features","text":"<p>\ud83d\udcf7 Photorealistic renderings of the virtual environment with pixel-perfect annotations</p> <p>\ud83d\udcc4 No-Code scene and sensor configuration with a simple YAML syntax</p> <p>\ud83d\udd27 Extensive randomization tools to increase the diversity of the generated data</p> <p>\ud83d\udcbe Asset management and viewer to easily reuse assets across multiple scenes</p> <p>\ud83d\udce6 Easy to use and extend with a modular architecture</p>"},{"location":"#annotations","title":"\ud83d\udd0d Annotations","text":"<p>Syclops supports a variety of annotated outputs for different use cases. The following outputs are currently supported:</p> Output Description RGB Rendered color image Semantic Segmentation Semantic segmentation mask with class ids Instance Segmentation Unique instance id for each object in the scene Depth Distance from the camera to each pixel Bounding Boxes Bounding boxes for each object in the scene Object Positions 3D position of each object in the scene Point Cloud 3D location of each pixel in camera space Keypoints Location of keypoints in camera space Object Volume Volume of each object in the scene Structured Light Projected dot pattern for structured light reconstruction"},{"location":"#terminology","title":"\ud83d\udce3 Terminology","text":"Term Description Scene A scene is a virtual 3D environment that is used by the pipeline to generate sensor data. Job A job is a YAML file that defines the environment, sensors, and output of a scene. Plugin A plugin is a python class that is used to extend the functionality of the pipeline. It can create new elements in the scene, simulate a sensor, or add a new output. Asset An asset is something that can be used by the pipeline. This can be a 3D model, a texture, or a material."},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<p>Necessary Software to install on your machine:</p> <ul> <li>\u2705 Tested Python Versions<ul> <li>Python 3.9 \u2013 3.11</li> </ul> </li> <li>\u274c Not yet compatible with Python 3.12+</li> </ul> <p>We recommend using a virtual environment to avoid potential package conflicts. Below are instructions for setting up with <code>virtualenv</code> or <code>conda</code>.</p>"},{"location":"getting_started/#installing","title":"Installing","text":"virtualenvconda <p>If you don't have <code>virtualenv</code> installed:</p> <pre><code>pip install virtualenv\n</code></pre> <p>To create and activate a new virtual environment named <code>syclops</code>:</p> WindowsLinux <pre><code>virtualenv syclops_venv\n.\\syclops_venv\\Scripts\\activate\n</code></pre> <pre><code>virtualenv syclops_venv\nsource syclops_venv/bin/activate\n</code></pre> <p>If you use Anaconda or Miniconda, you can create a new environment:</p> <pre><code>conda create --name syclops_venv python=3.9\nconda activate syclops_venv\n</code></pre>"},{"location":"getting_started/#installing-syclops","title":"Installing Syclops","text":"<p>Once you have your environment set up and activated:</p> From PyPIFrom Source <pre><code>pip install syclops\n</code></pre> <p>To install <code>Syclops</code> directly from the source code:</p> <pre><code>git clone https://github.com/DFKI-NI/syclops.git\ncd syclops\npip install .\n</code></pre> <p>Warning</p> <p><code>pip install . -e</code> does not work with the current setup.</p>"},{"location":"getting_started/#run-a-job","title":"Run a job","text":"<p>Next, the assets need to be crawled by the pipeline. This only needs to be done once, or if new assets are added. <pre><code>syclops -c\n</code></pre></p> <p>To run a job, a job file is needed. You can find an example in the syclops/__example_assets__ folder.</p> <p>To test the installation with the example job file run: <pre><code>syclops --example-job\n</code></pre></p> <p>To run a specific job, simply pass the path to the job file to the <code>syclops</code> command: <pre><code>syclops -j path/to/job.syclops.yaml\n</code></pre></p> <p>That's all you need to know to render images! \ud83c\udf89</p> <p>The rendered data will be in <code>output/&lt;timestamp&gt;</code> inside of your specified syclops directory. To quickly visuzalize the data, you can use the dataset viewer tool.</p> <p>Adjust the output path accordingly.</p> <pre><code>syclops -da output/2022-09-01_12-00-00\n</code></pre>"},{"location":"developement/architecture/","title":"Syclops Codebase Architecture","text":"<p>The following section provides an overview of the architectural structure of Syclops, helping developers and users understand the interplay between different components.</p>"},{"location":"developement/architecture/#repository-overview","title":"Repository Overview","text":"Component/Folder Description blender Contains the core code responsible for creating 3D scenes in Blender. It ensures that the virtual environment is rendered and prepared for data generation. blender/plugins Houses plugins that extend the pipeline's functionality. This includes plugins for the creation of a virtual scene, the lighting and objects. Both interface classes and their basic implementations can be found here. blender/sensors This folder houses the code used to add virtual sensors such as a camera to the scene. It also contains the interface class for simple extension of a new sensor. blender/sensor_outputs Contains the code for the sensor outputs. A sensor output is instanced by a sensor and is responsible for generating the data and ground truth annotation for that sensor. preprocessing Offers capabilities for altering the job description before the scene generation. It also generates the procedural textures for the scene. postprocessing Offers capabilities for altering the generated synthetic data post-creation. It allows users to refine or adjust the outputs based on their requirements."},{"location":"developement/architecture/#key-components","title":"Key Components:","text":"<p>The following is a list of relevant files:</p> <ul> <li>syclops<ul> <li>cli.py: Contains the code for the syclops CLI interface. It also orchestrates the preprocessing, postprocessing, and starting of Blender for the synthetic data generation.</li> </ul> </li> <li>syclops/preprocessing<ul> <li>preprocesor.py: Script that gets called before the scene generation. It adjusts the job description and can create additional files that are needed for the scene generation.</li> </ul> </li> <li>syclops/postprocessing<ul> <li>main.py: Entrypoint script that gets run as a parallel process to the Blender process. It is responsible for postprocessing the generated data.</li> <li>postprocessing_interface.py: Contains the interface class for the postprocessing. It defines, how a postprocessing plugin should be structured.</li> </ul> </li> <li>syclops/blender<ul> <li>main.py: Entrypoint script that gets run in Blender with the Blender Python environment.</li> <li>scene.py: Contains the code for creating the 3D scene in Blender. It creates the transformation tree, and loads the plugin/sensor instances that are defined in the job file.</li> </ul> </li> <li>syclops/blender/plugins<ul> <li>plugin_interface.py: Contains the interface class for the plugins. These are the classes that are used to generate objects or environments in the scene.</li> </ul> </li> <li>syclops/blender/sensors<ul> <li>sensor_interface.py: Contains the interface class for the sensors. These are the classes that are used to place virtual sensors in the scene.</li> </ul> </li> <li>syclops/blender/sensor_outputs<ul> <li>output_interface.py: Contains the interface class for the sensor outputs. A sensor output is instanced by a sensor and is responsible for generating the data for that sensor.</li> </ul> </li> </ul>"},{"location":"developement/debugging/","title":"Debugging","text":"<p>A variety a different debugging methods exist for syclops. They consist of either visually debugging a generated scene or IDE debugging of the python code.</p>"},{"location":"developement/debugging/#visually-debug-a-job-file","title":"\ud83d\udc1d Visually Debug a Job File","text":"<p>To better understand, how a scene looks inside of Blender, the <code>-d scene</code> flag can be used to break the pipeline at a certain point and open the Blender GUI.  <pre><code>syclops -c -j src/syclops-pipeline/examples/example_job.yaml -d scene\n</code></pre> The breakpoint is set inside the job file for a sensor output. The pipeline will break befor the rendering of that output occurs and cannot continue afterwards.</p> <p>In order to add a breakpoint, add the following attribute to a sensor output: <pre><code>debug_breakpoint: True\n</code></pre> Example of this is given in the <code>example_job.syclops.yaml</code> file (link). Blender will open right before the RGB image is rendered.</p> <p>Note</p> <p>Only one breakpoint can be set per job file. Other outputs before the breakpoint will be rendered, but the pipeline will stop after the breakpoint.</p>"},{"location":"developement/debugging/#code-debugging","title":"\ud83d\udc1c Code Debugging","text":"<p>Either the pipeline code or the Blender code can be debugged. It is strongly recommended to use VSCode for debugging, but other IDEs may work as well.</p> <p>To debug the code, add the <code>-d pipeline-code</code> or <code>-d blender-code</code> flag to the syclops command. Syclops will wait for a debugger to attach to the process on port 5678.</p> <p>Breakpoints have to be set in the code inside the install folder, since the src folder is not used for execution.</p> VSCodePyCharm <p>Note</p> <p>To attach VSCode to the process and start debugging, add the following configuration to the <code>launch.json</code> file and press F5.</p> <pre><code>{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n        \"name\": \"Syclops: Attach\",\n        \"type\": \"python\",\n        \"request\": \"attach\",\n        \"connect\": {\n          \"host\": \"localhost\",\n          \"port\": 5678\n            }\n        }\n    ]\n}\n</code></pre> <ol> <li>Open your project in PyCharm.</li> <li>Click on the Run menu and select Edit Configurations...</li> <li>In the left-hand menu, click on the + button and select Python Remote Debug from the dropdown menu.</li> <li>In the Configuration tab, enter a name for your configuration (e.g. <code>Syclops: Attach</code>).</li> <li>In the Debugger tab, ensure that the Localhost checkbox is selected under Address.</li> <li>Enter 5678 as the port number in the Port field.</li> <li>Click Apply and then OK to save the configuration.</li> </ol>"},{"location":"developement/add_functionality/create_plugin/","title":"Create a Scene Plugin","text":"<p>This document instructs users on how to create a plugin to enhance the functionality of Syclops.</p>"},{"location":"developement/add_functionality/create_plugin/#overview","title":"Overview","text":"<p>Plugins in Syclops are Python classes executed within Blender during synthetic data generation. They typically follow a structure that involves an initial setup (the load function) run once at the beginning, followed by a configuration step (the configure function) run for each rendered frame. This often pertains to importing 3D models initially and then adjusting their position and other properties for each step.</p> <p>Syclops provides an interface class to inherit from, ensuring sensor compatibility with the pipeline. This class contains the abstract methods load and configure, as well as other useful utility functions. Additionally, you can extend the plugin class with functions unique to the plugin's needs.</p>"},{"location":"developement/add_functionality/create_plugin/#basic-example","title":"Basic Example","text":"<p>Below is a basic example illustrating how a plugin can add a cube to the scene, attach it to a transformation tree node, and change its color using the config file.</p>"},{"location":"developement/add_functionality/create_plugin/#plugin","title":"Plugin","text":"cube.py<pre><code>import logging\nimport bpy # Blender python API\nfrom syclops import utility # Collection of useful utility functions\nfrom syclops.blender.plugins.plugin_interface import PluginInterface\n\nclass Cube(PluginInterface):\n    def __init__(self, config: dict):\n        super().__init__(config)\n\n    def load(self):\n        \"\"\"Add cube to the scene\"\"\"\n        # Create a Blender collection to store the cube in\n        collection = utility.create_collection(self.config[\"name\"]) \n        utility.set_active_collection(collection)\n\n        # Create a cube and add it to the collection\n        bpy.ops.mesh.primitive_cube_add() \n        cube_obj = bpy.context.object\n        # Create a material for the cube\n        material = bpy.data.materials.new(name=self.config[\"name\"] + \"_material\")\n        material.use_nodes = True\n        # Assign material to cube\n        cube_obj.data.materials.append(material)\n\n        self.objs.append(utility.ObjPointer(cube_obj)) # (1)!\n\n        self.setup_tf() # (2)!\n\n        logging.info(\"Cube: %s loaded\", self.config[\"name\"])\n\n\n    def configure(self):\n        \"\"\"Configure the cube for the current frame\"\"\"\n\n        # Set color of cube\n        cube_obj = self.objs[0].get() # (3)!\n        cube_material = cube_obj.data.materials[0]\n        rgba_value = utility.eval_param(self.config[\"color\"])\n        pbr_node = cube_material.node_tree.nodes[\"Principled BSDF\"]\n        pbr_node.inputs[\"Base Color\"].default_value = rgba_value\n\n        logging.info(\"Cube: %s configured\", self.config[\"name\"])\n</code></pre> <ol> <li>The blender cube object is stored as a pointer in order to be able to reference it later. This is necessary, because the \"cube_obj\" variable is not a safe reference to the object. To retrieve the object, the \"get\" function of the ObjPointer class is used.</li> <li>This function attaches all objects in self.objs to a transformation node, if \"frame_id\" is specified in the config file.</li> <li>Retrieve the cube object from the pointer</li> </ol>"},{"location":"developement/add_functionality/create_plugin/#plugin-registration","title":"Plugin Registration","text":"<p>To register your plugin with Syclops, add an entry in your pyproject.toml file under the [project.entry-points.\"syclops.plugins\"] section. For example:</p> pyproject.toml<pre><code>[project.entry-points.\"syclops.plugins\"]\nsyclops_cube_plugin = \"path.to.cube:Cube\"\n</code></pre> <p>Replace path.to.cube with the actual module path where your Cube class is defined.</p> <p>The plugin is now directly integrated into your package and recognized by Syclops through the pyproject.toml entry. Ensure your package is installed via pip in the same virtual environment as Syclops.</p> <p>Directory Structure:</p> <pre><code>.\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 plugins\n\u2502   \u251c\u2500\u2500 cube.py\n\u2502   \u2514\u2500\u2500 schema\n\u2502       \u2514\u2500\u2500 cube.schema.yaml\n</code></pre> <p>To install the plugin, simply install your package using pip. Once installed, Syclops will automatically detect and register the plugin. You can then use the plugin in a Syclops job configuration file within the <code>scene</code> section:</p> cube_example_job.syclops.yaml<pre><code># ...\ntransformations:\n    cube_node:\n        location: [0, 0, 0]\n        rotation: [0, 0, 0]\nscene:\n    syclops_cube_plugin: # (1)!\n        - name: cube1\n          frame_id: cube_node\n          color: [1, 0, 0, 1] # (2)!\n# ...\n</code></pre> <ol> <li>This is the name of the entry point defined in the pyproject.toml file.</li> <li>In order to randomize the RGBA value every frame, a dynamic evaluator can be used: <pre><code>color: \n    uniform: [[0, 0, 0, 1], [1, 1, 1, 1]]\n</code></pre></li> </ol>"},{"location":"developement/add_functionality/create_plugin/#schema","title":"Schema","text":"<p>A YAML schema file can be used to define the limits of the plugin configuration:</p> cube.schema.yaml<pre><code>description: A plugin that places a cube with a defined color in the scene\ntype: array # (1)!\nitems:\n    type: object\n    properties:\n        name:\n            type: string\n            description: Unique name of the cube\n        frame_id:\n            type: string\n            description: Name of the transformation node to attach the cube to\n        color:\n            type: array\n            items:\n                type: number\n                minimum: 0\n                maximum: 1\n            minItems: 4\n            maxItems: 4\n            description: RGBA color value\n    required: [name, frame_id, color]\n</code></pre> <ol> <li>Multiple instances of a plugin can be created. Therefore, the schema has to be of type array.</li> </ol>"},{"location":"developement/add_functionality/create_sensor/","title":"Create a Sensor and Output Plugins","text":"<p>In this document, the process of creating a new sensor with a new output for Syclops is explained.</p>"},{"location":"developement/add_functionality/create_sensor/#overview","title":"Overview","text":"<p>Sensor-Plugins in Syclops are Python classes, that mimic the behavior of real sensors in the virtual environment. They are treated as the foundation for the Output-Plugins, which are responsible for generating the data of the sensors. The sensor class is responsible for creating the sensor in the virtual environment and for providing the data to the Output-Plugin. The Output-Plugin is responsible for generating the data for the sensor. This allows to reuse the same sensor for different Output-Plugins.</p> <p>Syclops provides interface classes to inherit from, ensuring compatibility with the pipeline. For a sensor, the class has two abstract methods, that need to be implemented: setup_sensor and render_outputs. In the case of an output, the interface contains one abstract method: generate_output.</p>"},{"location":"developement/add_functionality/create_sensor/#basic-example","title":"Basic Example","text":"<p>Below is a basic example illustrating how a laser distance sensor can be added to Syclops. Since the sensor class itself does not generate any data, it is a fairly simple class. The rest of the functionality is implemented in the output plugin.</p>"},{"location":"developement/add_functionality/create_sensor/#sensor","title":"Sensor","text":"laser_distance_sensor.py<pre><code>import logging\nimport bpy  # Blender python API\nfrom syclops_blender import utility  # Collection of useful utility functions\nfrom syclops.blender.sensors.sensor_interface import SensorInterface\n\n\nclass LaserDistanceSensor(SensorInterface):\n    def setup_sensor(self):\n        \"\"\"Goal of this function is to create an object in the scene, that represents the sensor.\"\"\"\n        # Create empty object to represent the sensor\n        sensor_empty = bpy.data.objects.new(self.config[\"name\"], None)\n        sensor_coll = utility.create_collection(\"Sensors\")\n        sensor_coll.objects.link(sensor_empty)\n        # Add empty to class variable objs in order to be correctly attached to the tf-tree\n        self.objs.append(utility.ObjPointer(sensor_empty))  # (1)!\n\n        logging.info(\"Laser Distance Sensor: %s created\", self.config[\"name\"])\n\n    def render_outputs(self):\n        \"\"\"This function calls all outputs that are assigned to this sensor\"\"\"\n        for output in self.outputs:\n            output.generate_output(self)\n</code></pre> <ol> <li>The blender empty object is stored as a pointer in order to be able to reference it later. This is necessary, because the \"sensor_empty\" variable is not a safe reference to the object. To retrieve the object, the \"get\" function of the ObjPointer class is used.</li> </ol> <p>Warning</p> <p>This sensor class is in its current state not usable, because it does not generate any data. In order to use it, an output plugin has to be created.</p>"},{"location":"developement/add_functionality/create_sensor/#output","title":"Output","text":"laser_distance_output.py<pre><code>import json\nimport logging\nfrom pathlib import Path\n\nimport bpy\nfrom mathutils import Vector\nfrom syclops_blender import utility\nfrom syclops.blender.sensor_outputs.output_interface import OutputInterface\n\nMETA_DESCRIPTION = {\n    \"type\": \"DISTANCE\",\n    \"format\": \"JSON\",\n    \"description\": \"Distance of a single laser ray to the closest object in the scene\",\n}\n\n\nclass LaserDistanceOutput(OutputInterface):\n    \"\"\"Handles the generation of laser distance output for sensors.\"\"\"\n\n    def generate_output(self, parent_class: object):\n        \"\"\"Generate and save the laser ray distance output to JSON.\"\"\"\n        with utility.RevertAfter():  # (1)!\n            # Refresh the virtual scene\n            self._update_depsgraph()\n\n            # Get ray origin and direction\n            sensor_empty = parent_class.objs[0].get()\n            ray_origin, ray_direction = self._compute_ray(sensor_empty)\n\n            # Raycast\n            hit, location, _, _, obj_hit, _ = self._raycast(ray_origin, ray_direction)\n\n            distance = (location - ray_origin).length if hit else -1\n            class_id = obj_hit.get(\"class_id\") if hit else None\n\n            # Save output\n            output_path = self._prepare_output_folder(parent_class.config[\"name\"])\n            json_file = output_path / f\"{bpy.context.scene.frame_current:04}.json\"\n            self._save_output(json_file, distance, class_id, location, ray_origin)\n\n            # Write meta output\n            self.write_meta_output_file(json_file, parent_class.config[\"name\"])\n            logging.info(f\"Writing laser distance output to {json_file}\")\n\n    def _update_depsgraph(self):\n        \"\"\"Update the dependency graph.\"\"\"\n        bpy.context.view_layer.update()\n        utility.refresh_modifiers()\n\n    def _compute_ray(self, sensor_empty):\n        \"\"\"Compute the ray's origin and direction from the sensor_empty.\"\"\"\n        ray_origin = sensor_empty.matrix_world.translation\n        ray_direction = sensor_empty.matrix_world.to_quaternion() @ Vector(\n            (0.0, 0.0, -1.0)\n        )\n        return ray_origin, ray_direction\n\n    def _raycast(self, ray_origin, ray_direction):\n        \"\"\"Perform ray casting in the Blender scene.\"\"\"\n        return bpy.context.scene.ray_cast(\n            bpy.context.view_layer.depsgraph,\n            ray_origin,\n            ray_direction,\n        )\n\n    def _prepare_output_folder(self, sensor_name):\n        \"\"\"Prepare the output folder and return its path.\"\"\"\n        output_folder = utility.append_output_path(f\"{sensor_name}_annotations\")\n        utility.create_folder(output_folder)\n        return output_folder\n\n    def _save_output(self, json_file, distance, class_id, location, ray_origin):\n        \"\"\"Save the output data to a JSON file.\"\"\"\n        data = {\n            \"distance\": distance,\n            \"class_id\": class_id,\n            \"hit_location\": list(location),\n            \"origin\": list(ray_origin),\n        }\n        with open(json_file, \"w\") as f:\n            json.dump(data, f)\n\n    def write_meta_output_file(self, file: Path, sensor_name: str):\n        \"\"\"Write the metadata output to a YAML file.\"\"\"\n        output_path = file.parent\n\n        with utility.AtomicYAMLWriter(str(output_path / \"metadata.yaml\")) as writer:\n            writer.data.update(META_DESCRIPTION)\n            writer.add_step(\n                step=bpy.context.scene.frame_current,\n                step_dicts=[\n                    {\n                        \"type\": META_DESCRIPTION[\"type\"],\n                        \"path\": str(file.name),\n                    },\n                ],\n            )\n            writer.data[\"expected_steps\"] = utility.get_job_conf()[\"steps\"]\n            writer.data[\"sensor\"] = sensor_name\n            writer.data[\"id\"] = self.config[\"id\"]\n</code></pre> <ol> <li>The <code>RevertAfter</code> context manager is used to revert all changes made to the scene after the context manager is exited.</li> </ol>"},{"location":"developement/add_functionality/create_sensor/#sensor-and-ouput-registration","title":"Sensor and Ouput Registration","text":"<p>To register your sensor and output plugins with Syclops, add entries in your pyproject.toml file under the [project.entry-points.\"syclops.plugins\"] section. For example:</p> pyproject.toml<pre><code>[project.entry-points.\"syclops.sensors\"]\nsyclops_laser_sensor = \"path.to.laser_distance_sensor:LaserDistanceSensor\"\n[project.entry-points.\"syclops.outputs\"]\nsyclops_laser_output = \"path.to.laser_distance_output:LaserDistanceOutput\"\n</code></pre> <p>Replace <code>path.to.laser_distance_sensor</code> and <code>path.to.laser_distance_output</code> with the actual module paths where your <code>LaserDistanceSensor</code> and <code>LaserDistanceOutput</code> classes are defined.</p> <p>The plugins are now directly integrated into your package and recognized by Syclops through the pyproject.toml entry. Ensure your package is installed via pip in the same virtual environment as Syclops.</p> <p>Directory Structure: <pre><code>.\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 sensors\n\u2502   \u251c\u2500\u2500 laser_distance_sensor.py\n\u2502   \u2514\u2500\u2500 schema\n\u2502       \u2514\u2500\u2500 laser_distance_sensor.schema.yaml\n\u2514\u2500\u2500 outputs\n    \u251c\u2500\u2500 laser_distance_output.py\n    \u2514\u2500\u2500 schema\n        \u2514\u2500\u2500 laser_distance_output.schema.yaml\n</code></pre></p> <p>To install the plugins, simply install your package using pip. Once installed, Syclops will automatically detect and register the plugins. You can then use the plugins in a Syclops job configuration file within the <code>scene</code> section:</p> laser_distance_example_job.syclops.yaml<pre><code># ...\ntransformations:\n    laser_node:\n        location: [0, 0, 2]\n        rotation: [0, 0, 0]\nsensor:\n    syclops_laser_sensor: # (1)!\n        - name: laser_sensor\n          frame_id: laser_node\n          outputs:\n            - syclops_laser_output: # (1)!\n              id: laser_output\n# ...\n</code></pre> <ol> <li>This is the name of the entry point defined in the pyproject.toml file.</li> </ol>"},{"location":"developement/add_functionality/create_sensor/#schema","title":"Schema","text":"<p>A YAML schema file can be used to define the limits of the plugin configuration:</p> laser_distance_sensor.schema.yaml<pre><code>description: Frame for a laser distance sensor\ntype: array # (1)!\nitems:\n  type: object\n  properties:\n    name:\n      description: Unique identifier of the sensor\n      type: string\n    frame_id:\n      description: Transformation tree node to attach to.\n      type: string\n    outputs:\n      type: object\n      properties:\n        Docs Example Asset Library/LaserDistanceOutput:\n          $ref: \"#/definitions/Docs Example Asset Library/LaserDistanceOutput\" # (2)!\n      additionalProperties: False\n\n  required: [name, frame_id, outputs]\n</code></pre> <ol> <li>Multiple instances of a plugin can be created. Therefore, the schema has to be of type array.</li> <li>The schema of the output plugin is referenced here. This allows to define, which outputs are supported by the sensor.</li> </ol> laser_distance_output.schema.yaml<pre><code>description: Laser distance output. The distance and class_id are saved to a JSON file.\ntype: array # (1)!\nitems:\n  type: object\n  properties:\n    id:\n      description: Unique identifier of the output\n      type: string\n    debug_breakpoint:\n      description: Wether to break and open Blender before rendering. Only works if scene debugging is active.\n      type: boolean\n  required: [id]\n</code></pre> <ol> <li>Multiple instances of a plugin can be created. Therefore, the schema has to be of type array.</li> </ol>"},{"location":"usage/command_line/","title":"Command Line","text":"<p>To use <code>Syclops</code> the virtual environment needs to be activated. This can be done by running the following command in the terminal:</p> condavirtualenv <pre><code>conda activate syclops_venv\n</code></pre> <pre><code># For Windows\n./syclops_venv/Scripts/activate\n\n# For Linux\nsource syclops_venv/bin/activate\n</code></pre> <p>The most important alias is <code>syclops</code> which is the main command to use the pipeline. Following is a list of the command line arguments for <code>syclops</code>:</p> Argument Description Default Type <code>-j</code> Path to the job description file. <code>None</code> String <code>-o</code> Path to the output directory. Defaults to /output <code>./output</code> String <code>-c</code> Crawl assets and write a catalog file. <code>False</code> Boolean <code>--example-job</code> Run the example job config. <code>False</code> Boolean <code>-da</code> Path generated data to view. <code>False</code> String <code>-t</code> Command to generate thumbnails for the asset browser. <code>False</code> Boolean <code>-b</code> Start the asset browser. <code>False</code> Boolean <code>-log</code> Display all log messages in the console. <code>False</code> Boolean <code>-tv</code> Live display of the procedural textures in a job config. <code>None</code> String <code>-if</code> Path to the install folder. <code>None</code> String <code>-d</code> Debugging mode. See Debugging <code>False</code> String [scene, blender-code, pipeline-code]"},{"location":"usage/faq/","title":"Frequently Asked Questions","text":"<p>Here are some common questions and issues that users may encounter when using Syclops:</p>"},{"location":"usage/faq/#installation","title":"Installation","text":""},{"location":"usage/faq/#q-im-having-trouble-installing-syclops-what-should-i-do","title":"Q: I'm having trouble installing Syclops. What should I do?","text":"<p>A: Make sure you have the correct version of Python installed (3.9 or higher) and that you're using a virtual environment to avoid package conflicts. If you're still having issues, please open an issue on the GitHub repository with details about your operating system, Python version, and the error messages you're seeing.</p>"},{"location":"usage/faq/#assets","title":"Assets","text":""},{"location":"usage/faq/#q-how-do-i-add-new-assets-to-my-project","title":"Q: How do I add new assets to my project?","text":"<p>A: To add new assets, create an <code>assets.yaml</code> file in your project directory that defines the asset library and its assets. Then, run <code>syclops -c</code> to crawl the assets and update the catalog. For more information, see the Assets documentation.</p>"},{"location":"usage/faq/#q-im-getting-an-error-message-saying-an-asset-file-is-missing-what-should-i-do","title":"Q: I'm getting an error message saying an asset file is missing. What should I do?","text":"<p>A: Check that the file paths in your <code>assets.yaml</code> file are correct and that the files exist in the specified locations. If you've recently added or moved assets, make sure to run <code>syclops -c</code> to update the asset catalog.</p>"},{"location":"usage/faq/#job-configuration","title":"Job Configuration","text":""},{"location":"usage/faq/#q-my-job-configuration-isnt-working-as-expected-how-can-i-debug-it","title":"Q: My job configuration isn't working as expected. How can I debug it?","text":"<p>A: You can use the <code>-d</code> flag to enable debugging mode in Syclops. Use <code>-d scene</code> to open the scene in Blender for visual debugging, or <code>-d blender-code</code> and <code>-d pipeline-code</code> to debug the Blender and pipeline code, respectively. For more information, see the Debugging documentation.</p>"},{"location":"usage/faq/#q-how-do-i-use-dynamic-evaluators-in-my-job-configuration","title":"Q: How do I use dynamic evaluators in my job configuration?","text":"<p>A: Dynamic evaluators allow you to randomize parameter values for each frame in your scene. To use them, replace a fixed value in your job configuration with a dynamic evaluator expression, such as <code>uniform: [0, 1]</code> for a uniform random value between 0 and 1. For more examples, see the Dynamic Evaluators documentation.</p>"},{"location":"usage/faq/#rendering","title":"Rendering","text":""},{"location":"usage/faq/#q-my-renders-are-taking-a-long-time-how-can-i-speed-them-up","title":"Q: My renders are taking a long time. How can I speed them up?","text":"<p>A: To speed up rendering, you can try reducing the number of samples per pixel in your sensor configuration, or using a lower resolution for your output images. You can also make sure you're using GPU rendering if you have a compatible graphics card. For more tips, see the Sensor Configuration documentation.</p>"},{"location":"usage/faq/#q-im-getting-artifacts-or-noise-in-my-rendered-images-what-can-i-do","title":"Q: I'm getting artifacts or noise in my rendered images. What can I do?","text":"<p>A: Increase the number of samples per pixel in your sensor configuration to reduce noise and artifacts. You can also try enabling denoising in your job configuration by setting <code>denoising_enabled: True</code> and choosing an appropriate denoising algorithm, such as <code>OPTIX</code> or <code>OPENIMAGEDENOISE</code>.</p>"},{"location":"usage/faq/#postprocessing","title":"Postprocessing","text":""},{"location":"usage/faq/#q-how-do-i-create-custom-postprocessing-plugins","title":"Q: How do I create custom postprocessing plugins?","text":"<p>A: To create a custom postprocessing plugin, define a new Python class that inherits from <code>PostprocessorInterface</code> and implement the required methods, such as <code>process_step</code> and <code>process_all_steps</code>. Then, register your plugin in the <code>pyproject.toml</code> file under the <code>[project.entry-points.\"syclops.postprocessing\"]</code> section. For more details, see the Postprocessing documentation.</p> <p>If you have any other questions or issues that aren't covered here, please open an issue on the GitHub repository or reach out to the Syclops community for help.</p>"},{"location":"usage/assets/assets/","title":"Assets","text":"<p>In the Syclops context, assets can be a variety of things. This includes 3D models, textures or materials. The following sections will explain how to use assets in Syclops.</p>"},{"location":"usage/assets/assets/#asset-library","title":"Asset Library","text":"<p>Assets are usually grouped together as a library and defined in a YAML file. The following example shows how to define an asset library:</p> Example Asset Library<pre><code>name: Example Asset Library\ndescription: This is an example asset library\n\nassets:\n    My Example Object:\n        type: model\n        tags: [example]\n        filepath: my_first_asset.obj # (1)!\n    My Example Environment Texture:\n        type: environment_texture\n        tags: [sunny]\n        filepath: textures/my_sunny_environment.hdr # (2)!\n</code></pre> <ol> <li>Relative to the asset library file.</li> <li>Relative to the asset library file.</li> </ol> <p>Note</p> <p>Paths are always written relative to the asset library file.</p> <p>An asset is referenced by its library and unique name. Referencing the \"My Example Object\" would look like this: <code>Example Asset Library/My Example Object</code>.</p>"},{"location":"usage/assets/assets/#different-asset-types","title":"Different Asset Types","text":"ModelTexturePBR TextureEnvironment Texture <p>Models are used to create 3D objects in the scene. Model Asset<pre><code>&lt;asset_name&gt;:\n    type: model\n    filepath: &lt;path_to_model&gt;\n</code></pre></p> <p>Textures are mostly used to control things like density of scatter objects, height of the ground plane, or drive other parameters of the scene. Texture Asset<pre><code>&lt;asset_name&gt;:\n    type: texture\n    filepath: &lt;path_to_texture&gt;\n</code></pre></p> <p>PBR Textures are used for photorealistic rendering of objects. Currently they are only supported for the ground plane. PBR Texture Asset<pre><code>&lt;asset_name&gt;:\n    type: pbr_texture\n    texture_size: &lt;texture_size&gt; # (1)!\n    texture_displacement_scale: &lt;texture_displacement_scale&gt; # (2)!\n    diffuse_filepath: &lt;diffuse_texture_filepath&gt;\n    normal_filepath: &lt;normal_texture_filepath&gt;\n    roughness_filepath: &lt;roughness_texture_filepath&gt;\n    displacement_filepath: &lt;displacement_texture_filepath&gt;\n</code></pre></p> <ol> <li>The size of the texture in meters.</li> <li>Scale value to change effect of displacement map. Usually ~0.05.</li> </ol> <p>Environment Textures are used to light the scene and create a backdrop. Environment Texture Asset<pre><code>&lt;asset_name&gt;:\n    type: environment_texture\n    filepath: &lt;environment_texture_filepath&gt;\n</code></pre></p> <p>Info</p> <p>Each asset can have additional information that is either autogenerated or manually added. This includes the following attributes:</p> Attribute Description tags Additional tags for the object license License of the asset thumbnail List of thumbnail paths (autogenerated) *_md5 MD5 hash of the asset file (autogenerated)"},{"location":"usage/assets/assets/#installing-new-assets","title":"Installing New Assets","text":"<p>By running <code>syclops -c</code>, the pipeline crawls for asset library YAML files in all subdirectories of the paths defined in <code>&lt;install_dir&gt;/asset_paths.yaml</code>. Afterwards, a large asset catalog is created in the install directory. This catalog is used by the pipeline during runtime for asset lookup. With the catalog created, you can now reference assets in your scene files with <code>asset_library_name/asset_name</code>.</p> <p>Note</p> <p>The asset catalog is only updated when running <code>syclops -c</code>. If you add new assets, you have to run this command again.</p>"},{"location":"usage/job_description/dynamic_evaluators/","title":"Dynamic Evaluators","text":"<p>An important part of a synthetic dataset is variety. This increases the domain size and hopefully encapsulates the domain of the real dataset. In syclops, this is achieved by using dynamic evaluators that replace attributes in the job configuration, telling the pipeline which values and how to change them for each new image.</p> <p>A common randomization is the pose of the camera. The following example shows how this can be achieved.</p> Fixed camera pose<pre><code>camera_node:\n  location: [1, 0, 0] # Location fixed to [1, 0, 0]\n  rotation: [0, 0, 0] # Rotation fixed to [0, 0, 0]\n</code></pre> Random camera pose<pre><code>camera_node:\n  location:\n    # Location starts at [-5, 0, 0] and moves 0.5 units in x direction per step\n    linear: [[-5, 0, 0], [0.5, 0, 0]]\n  rotation:\n    # Rotation is normally distributed around [0.785398, 0, 0] with a standard deviation of [0.05, 0.05, 0.05]\n    normal: [[0.785398, 0, 0], [0.05, 0.05, 0.05]] \n</code></pre> <p>The attribute values were replaced with dynamic evaluators (<code>linear</code>, <code>normal</code>) which change the values for each new image. Most attributes can be randomized in this way, but some are fixed and cannot be changed.</p>"},{"location":"usage/job_description/dynamic_evaluators/#list-of-dynamic-evaluators","title":"List of Dynamic Evaluators","text":"<p>The following table lists all dynamic evaluators and their parameters.</p> Name Parameters Description linear <code>[&lt;start&gt;, &lt;step&gt;]</code> Linearly increases the value by <code>step</code> for each new image. normal <code>[&lt;mean&gt;, &lt;std&gt;]</code> Samples a value from a normal distribution with the given mean and standard deviation. uniform <code>[&lt;min&gt;, &lt;max&gt;]</code> Samples a value from a uniform distribution with the given minimum and maximum. step <code>[&lt;step1&gt;, &lt;step2&gt;, ...]</code> Samples a value from a list of steps in the given order. random_selection <code>[&lt;step1&gt;, &lt;step2&gt;, ...]</code> Samples a value from a list of steps in a random order. selection_asset <code>{library: &lt;library_name&gt;, type: &lt;type&gt;}</code> Randomly picks an asset from the given library and type. selection_wildcard <code>{library: &lt;library_name&gt;, wildcard: &lt;wildcard_pattern&gt;}</code> Randomly selects an asset from a library that matches the given wildcard pattern."},{"location":"usage/job_description/dynamic_evaluators/#referencing-global-evaluators","title":"Referencing Global Evaluators","text":"<p>In addition to the dynamic evaluators that are specific to each attribute, you can also reference global evaluators defined in the <code>global_evaluators</code> section of the job configuration. Global evaluators are evaluated once per frame and ensure that the same random value is used for all sensors within a single frame.</p> <p>To reference a global evaluator, use the syntax <code>$global.&lt;evaluator_name&gt;</code>. For example:</p> <pre><code>sensor:\n  syclops_sensor_camera:\n    - name: \"main_camera\"\n      gamma: $global.gamma\n      # ...\n    - name: \"secondary_camera\"\n      gamma: $global.gamma\n      # ...\n</code></pre> <p>In this example, both the <code>main_camera</code> and <code>secondary_camera</code> will use the same random value for <code>gamma</code> in each frame, as defined in the <code>global_evaluators</code> section.</p>"},{"location":"usage/job_description/job_configuration/","title":"Define a job","text":"<p>The entire pipeline is controlled and configured by a job. A job is a YAML file that defines the pipeline and its steps.</p>"},{"location":"usage/job_description/job_configuration/#job-structure","title":"Job structure","text":"<p>Each job can be divided into the following sections:</p> <ul> <li>general: General configuration that apply to Blender and the current hardware.</li> <li>transformations: Contains the transformation tree of objects and sensors in the scene.</li> <li>scene: Describes how the virtual environment should be created.</li> <li>sensor: Configuration of individual sensors and their corresponding outputs.</li> <li>postprocessing - [optional]: Operations that are applied to the generated data after creation.</li> <li>textures - [optional]: Dynamically generated textures that can be used in the scene.</li> <li>global_evaluators: Defines global evaluators that can be referenced by multiple plugins or sensors.</li> </ul> generaltransformationsscenesensorpostprocessingtexturesglobal_evaluators <p>General settings that configure Blender and the used hardware.</p> <p>Example: <pre><code>steps: 1 # Number of steps to render\nseeds:\n  numpy: 42 \n  cycles: 42\nrender_device: \"GPU\" # GPU or CPU\nrender_hardware: \"CUDA\" # CUDA or OPTIX\ndenoising_enabled: False\ndenoising_algorithm: \"OPTIX\" # OPTIX or OPENIMAGEDENOISE\n</code></pre></p> <p>Warning</p> <p>Setting CUDA or OPTIX on an unsupported GPU will result in an error. Only Nvidia RTX GPUs support OPTIX. To be safe, set the denoising_algorithm to OPENIMAGEDENOISE.</p> <p>Most 3D objects and sensors are positioned in the scene using a transformation tree. It is similar to a robot's kinematic tree. Each node in the tree has a unique name and can have multiple children. The transformation of a node is defined by its location and rotation. Objects are then later on linked to a specific node by their name.</p> StructureExample Basic transformations structure<pre><code>transformations:\n    &lt;name_of_node&gt;:\n        location: [x, y, z] # Location of the transformation in meters\n        rotation: [x, y, z] # Rotation of the transformation in radians\n        children: # Children of the transformation node\n            &lt;name_of_child_node&gt;:\n                location: [x, y, z] # Location of the transformation in meters\n                rotation: [x, y, z] # Rotation of the transformation in radians\n                children: # Children of the transformation node\n                    ...\n</code></pre> <p>Optionally, velocities can be defined for a node. This is only useful for dynamic sensor effects like motion blur. The pose is not effected or updated by the velocity. Transformation node with velocities<pre><code>&lt;name_of_node&gt;:\n    location: [x, y, z] # Location of the transformation in meters\n    rotation: [x, y, z] # Rotation of the transformation in radians\n    velocities:\n        translation: [x, y, z] # Translation velocity in meters per second\n        rotation: [x, y, z] # Rotation velocity in radians per second\n</code></pre></p> Basic transformation tree example<pre><code>transformations:\n  map: # ID of a transformation\n    location: [0, 0, 0] # Location of the transformation\n    rotation: [0, 0, 0] # Rotation of the transformation in radians\n    children:   # Children of the transformation node\n      camera_link:\n        location: [-20,0,2]\n        rotation: [0.785398, 0, 0]\n</code></pre> <p>Tip</p> <p>To alter the pose of a node, you can use the dynamic evaluators. See Dynamic Evaluators for more information.</p> <p>In this section, all elements of the virtual environment are defined. This includes the objects, lights, materials, and textures. For this, plugins are used that have some specific functionality to alter the scene. Each plugin has its own configuration options. The general structure of a scene is as follows:</p> StructureExample Basic scene structure<pre><code>scene:\n    &lt;plugin_name&gt;: # Name of the plugin\n        - &lt;plugin_configuration&gt; # Plugin specific configuration\n        - &lt;plugin_configuration&gt;\n        - ...\n    &lt;plugin_name&gt;:\n        - &lt;plugin_configuration&gt;\n        - ...\n</code></pre> Basic scene example<pre><code>scene:\nsyclops_plugin_ground: # Plugin that creates a ground plane\n    - name: \"Ground\"\n      size: 50 # Size of the ground rectangle in meters\n      texture: Example Assets/Muddy Dry Ground # Texture of the ground\n      class_id: 1 # Class ID of the ground\n\nsyclops_plugin_environment: # Plugin that creates a backdrop and environment lighting\n    - type: hdri\n      environment_image: Example Assets/Sunflower Field\n</code></pre> <p>Tip</p> <p>Most parameters of a plugin configuration can be dynamically altered in each new frame with dynamic evaluators. See Dynamic Evaluators for more information.</p> <p>Here, all sensors and their corresponding outputs are defined. Each sensor has its own configuration options. The general structure of a sensor is as follows:</p> StructureExample Basic sensor structure<pre><code>scene:\n    &lt;sensor_name&gt;: # Name of the sensor\n        - &lt;sensor_configuration&gt; # Sensor specific configuration\n        - &lt;sensor_configuration&gt;\n        - ...\n    &lt;sensor_name&gt;:\n        - &lt;sensor_configuration&gt;\n        - ...\n</code></pre> Basic scene example<pre><code>sensor:\n    syclops_sensor_camera: # Name of the sensor plugin\n        - name: \"main_camera\"\n        frame_id: \"camera_link\" # Name of the transformation node\n        resolution: [1280, 960] # Resolution of the sensor in pixels\n        focal_length: 65 # Focal length of the camera in mm\n        sensor_width: 35 # Sensor width of the camera in mm\n        exposure: 0.0 # Exposure (stops) shift of camera\n        gamma: 1.0 # Gamma correction applied to the image\n        outputs:\n            syclops_output_rgb:\n                - samples: 256\n                    id: main_cam_rgb\n            syclops_output_object_positions:\n                - id: main_cam_object_positions\n            syclops_output_pixel_annotation:\n                - semantic_segmentation:\n                    id: main_cam_semantic\n                  instance_segmentation:\n                    id: main_cam_instance\n                  pointcloud:\n                    id: main_cam_pointcloud\n                  depth:\n                    id: main_cam_depth\n                  object_volume:\n                    id: main_cam_object_volume\n</code></pre> <p>Each sensor is linked to a node of the transformation tree to define its pose. The most import attribute of a sensor configuration is its outputs. It contains which data the sensor should generate.</p> <p>Tip</p> <p>Most parameters of a plugin configuration can be dynamically altered in each new frame with dynamic evaluators. See Dynamic Evaluators for more information.</p> <p>Postprocessing operations are applied to the generated data after the scene generation. This can be used to refine or adjust the outputs based on the requirements.   The general structure of a postprocessing operation is as follows:</p> StructureExample Basic postprocessing structure<pre><code>postprocessing:\n    &lt;postprocessing_name&gt;: # Name of the postprocessing operation\n        - &lt;postprocessing_configuration&gt; # Postprocessing specific configuration\n        - &lt;postprocessing_configuration&gt;\n        - ...\n    &lt;postprocessing_name&gt;:\n        - &lt;postprocessing_configuration&gt;\n        - ...\n</code></pre> Basic postprocessing example<pre><code>postprocessing:\nsyclops_postprocessing_bounding_boxes:\n    - type: \"YOLO\"\n      classes_to_skip: [0, 1]\n      id: yolo_bound_boxes\n      sources: [\"main_cam_instance\", \"main_cam_semantic\"]\n</code></pre> <p>Textures are dynamically generated based on instructions in the job file during preprocessing. They can be used in the scene to add more variation to the generated data. A genereated texture can be referenced in the scene by its ID like this: <code>Preprocessed Assets/&lt;texture_id&gt;</code> The general structure of a texture is as follows:</p> StructureExample Basic texture structure<pre><code>textures:\n    &lt;texture_id&gt;: # ID of the texture\n        config: # Configuration of the texture\n            &lt;texture_configuration&gt;\n            &lt;texture_configuration&gt;\n            ...\n        ops: # Sequental operations that are applied to the texture\n            - &lt;texture_operation&gt;\n            - &lt;texture_operation&gt;\n            - ...\n\n    &lt;texture_id&gt;:\n        config:\n            &lt;texture_configuration&gt;\n            ...\n        ops:\n            - &lt;texture_operation&gt;\n            - ...\n</code></pre> Basic texture example<pre><code>textures:\nperlin_noise_tex_1:\n    config:\n    image_size: [512, 512]\n    bit_depth: 16\n    seed: 3\n    num_textures: 3\n    ops:\n    - perlin:\n        res: 8\n        octaves: 4\n</code></pre> <p>This section defines global evaluators that can be referenced by multiple plugins or sensor. These evaluators are evaluated once per frame and ensure that the same random value is used for all plugins/sensors within a single frame. This is useful for multiple cameras that should have the same random exposure value every frame.</p> <p>Tip</p> <p>See Dynamic Evaluators for more information.</p> <p>Example: <pre><code>global_evaluators:\n  gamma:\n    uniform: [0.8, 1.2]\n  exposure:\n    normal: [0, 1]\n</code></pre></p>"},{"location":"usage/job_description/validation/","title":"Validation","text":"<p>Syclops generates a YAML schema that defines the job configuration rules. This schema can be used for auto-completion and linting, but also to get more information about a specific property.</p> <p>During asset crawling, Syclops looks for all schema files of the plugins and creates a single <code>schema.yaml</code> in the install folder. In order to use the linting features for the job descriptions, the IDE has to be pointed towards the schema file.</p> Linting of job description VSCode <p>For Visual Studio Code, the YAML extension needs to be installed.   In the VSCode settings.json, the following entry has to be set. This will apply the schema to all YAML files ending with syclops.yaml.   <pre><code>  \"yaml.schemas\": {\n    \"&lt;path/to/schema.yaml&gt;\": \"*syclops.yaml\"\n  },\n</code></pre></p>"},{"location":"usage/job_description/config_descriptions/bounding_box/","title":"Postprocessing","text":"<p>Postprocessing operations are applied to the generated data after the scene rendering is complete. One common postprocessing task is generating bounding box annotations from the instance and semantic segmentation outputs.</p>"},{"location":"usage/job_description/config_descriptions/bounding_box/#bounding-box-generation","title":"Bounding Box Generation","text":"<p>The <code>syclops_postprocessing_bounding_boxes</code> plugin is used to generate bounding box annotations in the YOLO format from the instance and semantic segmentation images.</p> <pre><code>postprocessing:\n  syclops_postprocessing_bounding_boxes:\n    - type: \"YOLO\" \n      classes_to_skip: [0, 1] # List of class ids to exclude from bounding boxes\n      id: yolo_bound_boxes\n      sources: [\"main_cam_instance\", \"main_cam_semantic\"] # Names of instance and semantic outputs\n</code></pre> <p>The key parameters are:</p> <ul> <li><code>type</code>: The output format, in this case \"YOLO\" for the YOLO bounding box format.</li> <li><code>classes_to_skip</code>: A list of class ids to exclude from the bounding box generation.</li> <li><code>id</code>: A unique identifier for this postprocessing output.</li> <li><code>sources</code>: The names of the instance and semantic segmentation outputs to use as sources.</li> </ul>"},{"location":"usage/job_description/config_descriptions/bounding_box/#algorithm","title":"Algorithm","text":"<p>The bounding box generation algorithm works as follows:</p> <ol> <li>Load the instance and semantic segmentation images for the current frame.</li> <li>Create a mask of pixels to skip based on the <code>classes_to_skip</code> list.</li> <li>Find all unique remaining instance ids after applying the skip mask.</li> <li>For each instance id:<ul> <li>Find the class ids associated with that instance, excluding low pixel count classes.</li> <li>If <code>multiple_bb_per_instance</code> is enabled, generate one bounding box per class id.</li> <li>Otherwise, use the main class id and generate one bounding box.</li> </ul> </li> <li>Write the bounding boxes in YOLO format to an output file.</li> </ol> <p>The bounding box coordinates are calculated from the pixel extents of each instance mask for the given class id(s).</p>"},{"location":"usage/job_description/config_descriptions/bounding_box/#output","title":"Output","text":"<p>The bounding box output is generated as a text file in the YOLO format for each frame, located in the <code>&lt;sensor_name&gt;_annotations/bounding_box/</code> folder. Each line represents one bounding box:</p> <pre><code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\n</code></pre> <p>The coordinates are normalized between 0-1 based on the image width/height.</p>"},{"location":"usage/job_description/config_descriptions/camera/","title":"Camera Plugin Documentation","text":"<p>The Camera Plugin simulates a basic camera sensor, allowing you to configure the optical and digital properties of the camera within your scene. It supports various parameters such as resolution, lens types (perspective and fisheye), exposure, depth of field, motion blur, and frustum visualization for debugging purposes. Additionally, it outputs the intrinsic and extrinsic camera parameters.</p>"},{"location":"usage/job_description/config_descriptions/camera/#configuration-parameters","title":"Configuration Parameters","text":"<p>The following table describes each configuration parameter for the Camera Plugin:</p> Parameter Type Description Requirement <code>name</code> string Unique identifier of the sensor. Required <code>frame_id</code> string Transformation tree node the camera attaches to. Required <code>resolution</code> array (2 integers: width x height) Width and height of the camera in pixels. Required <code>lens_type</code> string Type of lens projection (PERSPECTIVE [default], FISHEYE_EQUISOLID, FISHEYE_EQUIDISTANT). Optional <code>focal_length</code> float Focal length of the camera in mm. Required for PERSPECTIVE and FISHEYE_EQUISOLID. Conditional <code>fisheye_fov</code> float Horizontal angular field of view in radians. Required for FISHEYE_EQUIDISTANT and FISHEYE_EQUISOLID. Conditional <code>sensor_width</code> float Width of the sensor in mm. Required <code>exposure</code> float Exposure offset in stops. Optional <code>gamma</code> float Gamma correction applied to the image (1 means no change in gamma). Optional <code>shutter_speed</code> float Shutter speed in seconds. Affects the strength of motion blur. If <code>motion_blur</code> is enabled, this becomes required. Conditional <code>depth_of_field</code> object (contains aperture, autofocus, focus_distance) Settings for the depth of field of the camera. Optional <code>motion_blur</code> object (contains enabled, rolling_shutter) Settings for the motion blur of the camera. Optional <code>frustum</code> object (contains settings for frustum visualization) Settings for the camera frustum visualization (for debugging purposes). Optional <code>outputs</code> object Output configuration, which can include RGB, Pixel Annotation, and Object Positions. Required"},{"location":"usage/job_description/config_descriptions/camera/#depth-of-field","title":"Depth of Field","text":"Sub-parameter Type Description <code>aperture</code> number f-number of the aperture. <code>autofocus</code> boolean Whether the camera should focus on the object in the center of the image. <code>focus_distance</code> number Fixed focus distance of the camera in m."},{"location":"usage/job_description/config_descriptions/camera/#motion-blur","title":"Motion Blur","text":"Sub-parameter Type Description <code>enabled</code> boolean Whether motion blur is enabled. <code>rolling_shutter</code> object Contains parameters for the rolling shutter effect."},{"location":"usage/job_description/config_descriptions/camera/#rolling-shutter","title":"Rolling Shutter","text":"Sub-parameter Type Description <code>enabled</code> boolean Whether rolling shutter is enabled. <code>duration</code> number Exposure time of the scanline in seconds."},{"location":"usage/job_description/config_descriptions/camera/#frustum-visualization","title":"Frustum Visualization","text":"Sub-parameter Type Description <code>enabled</code> boolean Whether to enable frustum visualization. <code>type</code> string Type of frustum visualization (e.g., \"pyramid\"). <code>depth</code> number Depth of the frustum in meters. <code>color</code> array RGB color of the frustum as a list of 3 floats. <code>transparency</code> number Transparency value between 0-1. <code>wireframe</code> object Settings for wireframe rendering mode. <code>hide_render</code> boolean Whether to hide the frustum in the final rendered images."},{"location":"usage/job_description/config_descriptions/camera/#wireframe-settings","title":"Wireframe Settings","text":"Sub-parameter Type Description <code>enabled</code> boolean Whether to render as wireframe lines. <code>thickness</code> number Thickness of the wireframe lines."},{"location":"usage/job_description/config_descriptions/camera/#lens-types-and-parameters","title":"Lens Types and Parameters","text":"<p>The camera supports three types of lens projections:</p> <ol> <li>PERSPECTIVE (default)</li> <li>Requires <code>focal_length</code></li> <li> <p>Standard perspective projection</p> </li> <li> <p>FISHEYE_EQUISOLID</p> </li> <li>Requires both <code>focal_length</code> and <code>fisheye_fov</code></li> <li>Follows the equisolid angle projection formula</li> <li> <p>Commonly used in real fisheye lenses</p> </li> <li> <p>FISHEYE_EQUIDISTANT</p> </li> <li>Requires <code>fisheye_fov</code></li> <li>Linear mapping between angle and image distance</li> <li>Theoretical fisheye projection</li> </ol> <p>Note</p> <p>When using fisheye lens types, the frustum visualization is not supported and will be disabled automatically. Camera extrinsic parameters are not output when using fisheye lens types as they are not supported.</p> <p>Warning</p> <p>If <code>motion_blur</code> is enabled, <code>shutter_speed</code> becomes a required parameter.</p> <p>Tip</p> <p>The frustum visualization is primarily intended for debugging purposes when running Syclops with the <code>-d scene</code> flag. This flag opens the scene in Blender and allows you to visualize the frustum of the sensor, which can be useful for sensor placement prototyping.</p>"},{"location":"usage/job_description/config_descriptions/camera/#intrinsic-camera-parameters-output","title":"Intrinsic Camera Parameters Output","text":"<p>The Camera Plugin outputs the intrinsic camera parameters, which include the camera matrix. The camera matrix is written to a YAML file named <code>&lt;frame_number&gt;.yaml</code> in the <code>&lt;camera_name&gt;/intrinsics</code> folder for each frame.</p>"},{"location":"usage/job_description/config_descriptions/camera/#example-intrinsics-output","title":"Example Intrinsics Output","text":"<pre><code>camera_matrix:\n  - [fx, 0, cx]\n  - [0, fy, cy] \n  - [0, 0, 1]\n</code></pre> <p>Where: - <code>fx</code>, <code>fy</code>: Focal lengths in pixels - <code>cx</code>, <code>cy</code>: Principal point coordinates in pixels</p>"},{"location":"usage/job_description/config_descriptions/camera/#extrinsic-camera-parameters-output","title":"Extrinsic Camera Parameters Output","text":"<p>The Camera Plugin also outputs the extrinsic camera parameters, which represent the global pose of the camera in the scene. The camera pose is written to a YAML file named <code>&lt;frame_number&gt;.yaml</code> in the <code>&lt;camera_name&gt;/extrinsics</code> folder for each frame.</p>"},{"location":"usage/job_description/config_descriptions/camera/#example-extrinsics-output","title":"Example Extrinsics Output","text":"<pre><code>camera_pose:\n  - [r11, r12, r13, tx]\n  - [r21, r22, r23, ty]\n  - [r31, r32, r33, tz]\n  - [0, 0, 0, 1]\n</code></pre> <p>Where: - <code>r11</code> to <code>r33</code>: Rotation matrix elements - <code>tx</code>, <code>ty</code>, <code>tz</code>: Translation vector elements</p>"},{"location":"usage/job_description/config_descriptions/camera/#metadata-output","title":"Metadata Output","text":"<p>Along with the intrinsic and extrinsic parameter files, a <code>metadata.yaml</code> file is generated in the respective output folders. This file contains metadata about the parameter outputs, including the output type, format, description, expected steps, sensor name, and output ID.</p>"},{"location":"usage/job_description/config_descriptions/camera/#example-configuration","title":"Example Configuration","text":"<pre><code>syclops_sensor_camera:\n  - name: Main_Camera\n    frame_id: Camera_Node_01\n    resolution: [1920, 1080]\n    focal_length: 35\n    sensor_width: 36\n    exposure: 0.5\n    gamma: 1.2\n    depth_of_field:\n      aperture: 2.8\n      autofocus: true\n    motion_blur:\n      enabled: true\n      rolling_shutter:\n        enabled: true\n        duration: 0.001\n    frustum:\n      enabled: true\n      type: pyramid\n      depth: 10\n      color: [1, 0, 0]\n      transparency: 0.5\n      wireframe:\n        enabled: true\n        thickness: 0.1\n    outputs:\n        syclops_output_rgb:\n            - samples: 256\n              id: main_cam_rgb\n</code></pre> <p>In the example above, a camera named \"Main_Camera\" is defined with a resolution of 1920x1080 pixels, a focal length of 35mm, and other specific properties. The camera will also utilize motion blur with a rolling shutter effect. Additionally, the frustum visualization is enabled, displaying a wireframe pyramid with a depth of 10 meters, colored red, and semi-transparent. The intrinsic and extrinsic camera parameters will be output according to the specified configuration.</p>"},{"location":"usage/job_description/config_descriptions/crop/","title":"Crop Plugin Documentation","text":"<p>The Crop Plugin scatters 3D models on a floor object in a grid pattern to simulate crop rows. </p>"},{"location":"usage/job_description/config_descriptions/crop/#configuration-parameters","title":"Configuration Parameters","text":"<p>The following table describes each configuration parameter for the Crop Plugin:</p> Parameter Type Description Requirement <code>name</code> string Unique identifier of the plugin Required <code>models</code> array or single item 3D assets to scatter. Required <code>floor_object</code> string Name of the floor object to scatter on. Required <code>max_texture_size</code> integer Maximum texture size in pixel. Will reduce the texture to save GPU RAM. Optional <code>density_map</code> image/texture evaluation Texture that alters the density. It is normalized to 0-1. Optional <code>decimate_mesh_factor</code> number (0-1) Factor between 0-1 that decimates the number of vertices of the mesh. Lower means less vertices. Optional <code>scale_standard_deviation</code> number evaluation Scale variance of the scattered objects. Required <code>class_id</code> integer Class ID for ground truth output. Required <code>crop_angle</code> number evaluation Global orientation of the row direction in degrees. Required <code>row_distance</code> number evaluation Distance between rows in meters. Required <code>row_standard_deviation</code> number evaluation Standard deviation of the row distance in meters. Required <code>plant_distance</code> number evaluation Intra row distance between plants in meters. Required <code>plant_standard_deviation</code> number evaluation Standard deviation of the intra row distance in meters. Required"},{"location":"usage/job_description/config_descriptions/crop/#dynamic-evaluators","title":"Dynamic Evaluators","text":"<p>Most parameters, like <code>scale_standard_deviation</code>, <code>crop_angle</code> etc., can be dynamically evaluated. This means that their values can be altered for each new frame. For more insights on dynamic evaluators and how to use them, kindly refer to Dynamic Evaluators.</p>"},{"location":"usage/job_description/config_descriptions/crop/#example-configuration","title":"Example Configuration","text":"<pre><code>scene:  \n  syclops_plugin_crop:\n    - name: \"Corn Crop\"\n      models: Example Assets/Corn\n      floor_object: \"Ground\"\n      max_texture_size: 2048\n      scale_standard_deviation: 0.1\n      class_id: 2\n      crop_angle: 45\n      row_distance: 1\n      row_standard_deviation: 0.1\n      plant_distance: 0.3  \n      plant_standard_deviation: 0.05\n</code></pre> <p>The above configuration will scatter corn models across the ground surface in a grid pattern resembling crop rows. The rows will be oriented at a 45 degree angle, with 1 meter spacing between rows and 30 cm spacing between plants. The row and plant spacings will vary according to the specified standard deviations.</p>"},{"location":"usage/job_description/config_descriptions/environment/","title":"Environment Plugin Documentation","text":"<p>The Environment Plugin is designed to set the ambiance of the scene. It establishes the lighting and background elements. By randomly changing the environment, the colors drastically shift and adds variety to the generated data.</p>"},{"location":"usage/job_description/config_descriptions/environment/#configuration-parameters","title":"Configuration Parameters","text":"<p>The following table describes each configuration parameter for the Environment Plugin:</p> Parameter Type Description Requirement <code>type</code> string (enum: <code>hdri</code>, <code>hdri_and_sun</code>) Determines the type of environment setup. Choose from solely HDRI or a combination of HDRI with sun. Required <code>environment_image</code> string (dynamic evaluator capable) The HDRI image used as the environment map, serving as both the background and the illumination source. Required <code>sun_elevation</code> number (dynamic evaluator capable) The elevation angle of the sun, defined in radians. Required if <code>type</code> is <code>hdri_and_sun</code> <code>sun_rotation</code> number (dynamic evaluator capable) The sun's rotation angle, measured in radians. Required if <code>type</code> is <code>hdri_and_sun</code> <code>random_rotation</code> boolean Randomly rotates the environment map every frame. Optional (default: <code>true</code>) <p>Warning</p> <p>When you choose <code>hdri_and_sun</code> for the <code>type</code>, you must also provide values for <code>sun_elevation</code> and <code>sun_rotation</code>.</p>"},{"location":"usage/job_description/config_descriptions/environment/#dynamic-evaluators","title":"Dynamic Evaluators","text":"<p>Parameters like <code>environment_image</code>, <code>sun_elevation</code>, and <code>sun_rotation</code> can be set to change dynamically in every frame. This is handy for simulating the motion of the environment or sun over time. To understand how to apply dynamic evaluators to these parameters, please refer to Dynamic Evaluators.</p>"},{"location":"usage/job_description/config_descriptions/environment/#example-configuration","title":"Example Configuration","text":"<p>Here's a sample configuration for the Environment Plugin:</p> <pre><code>scene:\n  syclops_plugin_environment:\n    - type: hdri\n      environment_image: Example Assets/Sunflower Field\n</code></pre> <p>In the above setup, the scene will have a sunflower field HDRI backdrop.</p>"},{"location":"usage/job_description/config_descriptions/ground/","title":"Ground Plugin Documentation","text":"<p>The Ground Plugin is a fundamental asset that lets you set up the floor plane for the virtual scene. With the flexibility to assign different materials and height maps, the Ground Plugin offers a realistic baseline for the virtual environment. </p>"},{"location":"usage/job_description/config_descriptions/ground/#overview","title":"Overview","text":"<p>The Ground Plugin is responsible for creating a square floor in the virtual environment. Most other plugins rely on the ground plane for object placement. Depending on your needs, this floor can be designed using predefined 3D models, or you can create a floor plane from scratch by specifying its size and texture.</p>"},{"location":"usage/job_description/config_descriptions/ground/#configuration-parameters","title":"Configuration Parameters","text":"<p>The following table describes each configuration parameter for the Ground Plugin:</p> Parameter Type Description Requirement <code>name</code> string A unique identifier for the plugin instance. Required <code>object_path</code> asset model The path to a 3D Object representing the ground. If used, <code>size</code>, <code>texture</code>, and <code>displacement_texture</code> must not be specified. Optional <code>size</code> number Specifies the width and length of the ground in meters. This is used when generating a floor plane without a 3D model. Optional <code>texture</code> string (dynamic evaluator capable) The PBR texture of the ground. This is required if you're creating a floor plane without using a 3D model. Optional <code>displacement_texture</code> image texture (dynamic evaluator capable) A height map for the ground. If the texture is 16-bit, a pixel value of 32768 signifies 0m in height. An increase in pixel value by 1 corresponds to a height difference of 0.5cm. Optional <code>class_id</code> integer Class ID for ground truth output. Required <p>Warning</p> <p>When using <code>object_path</code>, you must not specify <code>size</code>, <code>texture</code>, and <code>displacement_texture</code>. Conversely, when specifying <code>size</code> and <code>texture</code>, the <code>object_path</code> parameter must not be used.</p>"},{"location":"usage/job_description/config_descriptions/ground/#dynamic-evaluators","title":"Dynamic Evaluators","text":"<p>Some parameters, like <code>texture</code> and <code>displacement_texture</code>, can be dynamically evaluated. This means that their values can be altered for each new frame. For more insights on dynamic evaluators and how to use them, kindly refer to Dynamic Evaluators.</p>"},{"location":"usage/job_description/config_descriptions/ground/#example-configuration","title":"Example Configuration","text":"<p>Here's a simple example of how to configure the Ground Plugin:</p> <pre><code>scene:\n  syclops_plugin_ground:\n    - name: \"Ground\"\n      size: 50\n      texture: Example Assets/Muddy Dry Ground\n      displacement_texture: Example Assets/Ground Displacement 1\n      class_id: 1\n</code></pre> <p>In the example above, a square floor plane of 50 meters with a muddy dry ground texture and a height map is created.</p>"},{"location":"usage/job_description/config_descriptions/keypoint_annotation/","title":"Keypoint Output Documentation","text":"<p>The Keypoint Output is designed to provide the 2D pixel coordinates of predefined keypoints on 3D objects in the camera space. This output is particularly useful for tasks such as pose estimation, tracking, and analysis of object articulation.</p>"},{"location":"usage/job_description/config_descriptions/keypoint_annotation/#keypoint-definition","title":"Keypoint Definition","text":"<p>Keypoints are defined using a Blender script that allows users to easily add keypoint information to 3D objects. To use it, open Blender with the model and paste the script in the Blender text editor. The script can be used in two ways:</p> <ol> <li> <p>The user should create empty objects at the desired keypoint locations relative to the mesh object. Then, select all the empty objects and the mesh object, with the mesh object being the active object. Run the script, and it will add the keypoint information to the mesh object based on the positions of the empty objects. The empty objects will be sorted alphabetically, and their index will be used as the keypoint number.</p> </li> <li> <p>With a single mesh object selected that already has a keypoints attribute: The script will create empty objects at the keypoint positions defined in the mesh object to visualize the keypoint locations.</p> </li> </ol> <p>Here's an example of how the keypoints are stored in the mesh object:</p> <pre><code>obj[\"keypoints\"] = {\n    \"0\": {\"x\": -0.5, \"y\": 1.0, \"z\": 0.0},\n    \"1\": {\"x\": 0.5, \"y\": 1.0, \"z\": 0.0},\n    \"2\": {\"x\": 0.0, \"y\": 1.5, \"z\": 0.0},\n    ...\n}\n</code></pre> <p>Each keypoint is represented by a unique index (based on the alphabetical order of the empty objects) and its 3D coordinates relative to the object's local space.</p>"},{"location":"usage/job_description/config_descriptions/keypoint_annotation/#output-format","title":"Output Format","text":"<p>The keypoint output is saved as a JSON file for each frame, with the following structure:</p> <pre><code>{\n  \"instance_id_1\": {\n    \"class_id\": 1,\n    \"0\": {\n      \"x\": 100,\n      \"y\": 200\n    },\n    \"1\": {\n      \"x\": 150,\n      \"y\": 220\n    }\n  },\n  \"instance_id_2\": {\n    \"class_id\": 2,\n    \"0\": {\n      \"x\": 300,\n      \"y\": 400\n    },\n    \"1\": {\n      \"x\": 350,\n      \"y\": 420\n    }\n  }\n}\n</code></pre> <p>Each object instance is identified by a unique <code>instance_id</code>, which is calculated based on the object's 3D location. The <code>class_id</code> represents the semantic class of the object. Each keypoint is then listed with its index and 2D pixel coordinates (<code>x</code>, <code>y</code>) in the camera space.</p>"},{"location":"usage/job_description/config_descriptions/keypoint_annotation/#configuration-parameters","title":"Configuration Parameters","text":"<p>The keypoint output does not require any additional configuration parameters beyond the standard <code>id</code> field for uniquely identifying the output.</p> Parameter Type Description Requirement <code>id</code> string Unique identifier of the keypoint output. Required"},{"location":"usage/job_description/config_descriptions/keypoint_annotation/#example-configuration","title":"Example Configuration","text":"<pre><code>syclops_output_keypoint:\n  - id: \"keypoints1\"\n</code></pre> <p>In this example, a keypoint output is configured with the identifier <code>\"keypoints1\"</code>.</p>"},{"location":"usage/job_description/config_descriptions/keypoint_annotation/#metadata-output","title":"Metadata Output","text":"<p>Along with the keypoint JSON files, a <code>metadata.yaml</code> file is generated in the output folder. This file contains metadata about the keypoint output, including the output type, format, description, expected steps, sensor name, and output ID.</p>"},{"location":"usage/job_description/config_descriptions/keypoint_annotation/#limitations-and-considerations","title":"Limitations and Considerations","text":"<ul> <li>Keypoints are only generated if they are visible in the rendered image.</li> <li>The accuracy of keypoint locations depends on the precision of their definition in the 3D object space.</li> <li>Keypoint outputs are generated per frame, so the number of output files will depend on the total number of frames in the animation.</li> </ul> <p>By leveraging the keypoint output, users can obtain precise 2D locations of predefined keypoints on 3D objects, enabling various downstream tasks that require spatial understanding of object parts and their relationships.</p>"},{"location":"usage/job_description/config_descriptions/object/","title":"Object Plugin Documentation","text":"<p>The Object Plugin facilitates the addition of individual 3D models within the scene. It's essential to note that the positioning (pose) of these objects is managed by the transformation tree.</p>"},{"location":"usage/job_description/config_descriptions/object/#configuration-parameters","title":"Configuration Parameters","text":"<p>The following table describes each configuration parameter for the Object Plugin:</p> Parameter Type Description Requirement <code>name</code> string A unique identifier for the plugin instance. Required <code>models</code> array or single item 3D models to be placed in the scene. It can be a list or a single item of 3D assets. Required <code>frame_id</code> string Denotes the node in the transformation tree where the object is attached. Required <code>place_on_ground</code> boolean If set to <code>true</code>, the object will be positioned on the ground and aligned to its normal. Optional (Default: <code>false</code>) <code>floor_object</code> string Specifies the ground object's name on which the model is placed. It's mandatory if <code>place_on_ground</code> is true. Required if <code>place_on_ground</code> is true <code>max_texture_size</code> integer The texture's upper limit in pixels. Useful for reducing texture size and conserving GPU memory. Optional <code>decimate_mesh_factor</code> number (0-1) The factor to reduce the number of mesh vertices. A lower value means fewer vertices. Optional <code>class_id</code> integer Used for specifying the Class ID in the ground truth output. Required <p>Warning</p> <p>When setting the <code>place_on_ground</code> parameter to <code>true</code>, ensure you also specify the <code>floor_object</code>.</p>"},{"location":"usage/job_description/config_descriptions/object/#dynamic-evaluators","title":"Dynamic Evaluators","text":"<p>For parameters that can undergo dynamic evaluation, their values can be adjusted for every frame, offering flexibility in simulation. More on this can be found in the Dynamic Evaluators section.</p>"},{"location":"usage/job_description/config_descriptions/object/#example-configuration","title":"Example Configuration","text":"<pre><code>scene:\n  syclops_plugin_object:\n    - name: \"Tree1\"\n      models: Example Assets/Tree\n      frame_id: \"frame_001\"\n      place_on_ground: true\n      floor_object: \"Ground\"\n      max_texture_size: 1024\n      decimate_mesh_factor: 0.5\n      class_id: 2\n</code></pre> <p>The configuration places a tree model on the ground, adjusts its texture size and reduces its vertices, also assigning a class ID.</p>"},{"location":"usage/job_description/config_descriptions/object_position/","title":"Object Positions Output Documentation","text":"<p>The Object Positions Output provides information about the global position, rotation and scale of all objects within the scene. Each object's details are cataloged with their respective class IDs, their xyz coordinates (in meters) xyz euler angles (in radians) and xyz scale. This information is presented in a JSON format. The structure is the following:</p> <pre><code>{\n    \"&lt;class_id&gt;\": [\n      {\n        \"loc\": [x, y, z], // location in meters\n        \"rot\": [x, y, z], // euler in radians\n        \"scl\": [x, y, z]  // scale\n      },\n      ...\n    ]\n}\n</code></pre>"},{"location":"usage/job_description/config_descriptions/object_position/#configuration-parameters","title":"Configuration Parameters","text":"<p>The following table describes each configuration parameter for the Object Positions Output:</p> Parameter Type Description Requirement <code>id</code> string Unique identifier of the output. Required <code>debug_breakpoint</code> boolean Specifies whether to pause and open Blender before rendering. This functionality is available only when scene debugging is active. Optional <p>Note</p> <p>For each Object Positions Output configuration, an <code>id</code> is mandatory. Ensure the uniqueness of this identifier across different outputs.</p>"},{"location":"usage/job_description/config_descriptions/object_position/#example-configuration","title":"Example Configuration","text":"<pre><code>syclops_output_object_positions:\n  - id: \"obj_pos_1\"\n    debug_breakpoint: true\n</code></pre> <p>In the example configuration, the global positions of objects within the scene will be captured with the identifier <code>obj_pos_1</code>. Additionally, if the scene debugging is active, the scene will break and open in Blender before rendering.</p>"},{"location":"usage/job_description/config_descriptions/object_position/#metadata-output","title":"Metadata Output","text":"<p>Along with the output files, a <code>metadata.yaml</code> file is generated in the output folder. This file contains metadata about the keypoint output, including the output type, format, description, expected steps, sensor name, and output ID.</p>"},{"location":"usage/job_description/config_descriptions/pixel_annotation/","title":"Pixel Annotation Output Documentation","text":"<p>The Pixel Annotation Output is dedicated to providing various pixel-level annotations of the sensor image. This encompasses a range of annotations from semantic segmentation to the volume of objects.</p>"},{"location":"usage/job_description/config_descriptions/pixel_annotation/#inter-class-segmentation","title":"Inter Class Segmentation","text":"<p>In Syclops it is possible to have multiple class labels for a single object. This means, that a plant can have the segmentation labels <code>stem</code> and <code>leaf</code> at the same time. </p> <p>It has to be configured in the scene description for the object that should have multiple labels. The following example shows how to configure it:</p> Configure Inter Class Segmentation<pre><code>  syclops_plugin_scatter:\n    - name: \"Corn Scatter\"\n      ...\n      class_id: 2 # (1)!\n      class_id_offset:\n        Stem: 1 # (2)!\n      ...\n</code></pre> <ol> <li>Base class label for the object.</li> <li>Offset for the material <code>Stem</code>.</li> </ol> <p>This will result in the scattered corn objects to have the class label 2 for the whole object and the class label 3 for the part of the object that has the material <code>Stem</code> assigned.</p>"},{"location":"usage/job_description/config_descriptions/pixel_annotation/#configuration-parameters","title":"Configuration Parameters","text":"<p>The following table describes each configuration parameter for the Pixel Annotation Output:</p> Parameter Type Description Requirement <code>semantic_segmentation</code> object Represents the semantic segmentation output where pixels are mapped with the class id value of the object. Optional \u21b3 <code>id</code> string Unique identifier of the output. Required for this annotation type <code>instance_segmentation</code> object Produces an instance segmentation output, tagging each object with a unique id in the image. Optional \u21b3 <code>id</code> string Unique identifier of the output. Required for this annotation type <code>pointcloud</code> object Offers 3D coordinates of every pixel in the camera coordinates in meters. Optional \u21b3 <code>id</code> string Unique identifier of the output. Required for this annotation type <code>depth</code> object Displays the Z Depth of a pixel relative to the camera in meters. Optional \u21b3 <code>id</code> string Unique identifier of the output. Required for this annotation type <code>object_volume</code> object Shows the volume of objects in cm^3. Optional \u21b3 <code>id</code> string Unique identifier of the output. Required for this annotation type <code>debug_breakpoint</code> boolean Decides if the rendering process should pause and open Blender before proceeding. Only functions with scene debugging active. Optional <p>Note</p> <p>Ensure that each annotation type, if used, contains a unique <code>id</code>. The <code>id</code> is imperative for differentiating between various annotations.</p>"},{"location":"usage/job_description/config_descriptions/pixel_annotation/#example-configuration","title":"Example Configuration","text":"<pre><code>syclops_output_pixel_annotation:\n  - semantic_segmentation:\n      id: \"seg1\"\n      class_id_offset: true\n  - instance_segmentation:\n      id: \"inst1\"\n  - pointcloud:\n      id: \"pc1\"\n  - depth:\n      id: \"depth1\"\n  - debug_breakpoint: true\n</code></pre> <p>In the provided configuration, a variety of pixel annotations are set up, each with their unique identifiers. Additionally, if the scene debugging is active, the scene will break and open in Blender before rendering.</p>"},{"location":"usage/job_description/config_descriptions/pixel_annotation/#metadata-output","title":"Metadata Output","text":"<p>Along with the output files, a <code>metadata.yaml</code> file is generated in the output folder. This file contains metadata about the keypoint output, including the output type, format, description, expected steps, sensor name, and output ID.</p>"},{"location":"usage/job_description/config_descriptions/rgb/","title":"RGB Output Plugin Documentation","text":"<p>The RGB Output Plugin captures the RGB color output of the camera sensor and delivers photorealistic images. The generated output can range in quality, based on the number of samples per pixel. Lower samples per pixel will result in a grainy image, while higher samples per pixel will result in a smoother image.</p>"},{"location":"usage/job_description/config_descriptions/rgb/#configuration-parameters","title":"Configuration Parameters","text":"<p>The following table describes each configuration parameter for the RGB Output Plugin:</p> Parameter Type Description Requirement <code>id</code> string A unique identifier for the output. Required <code>samples</code> integer Specifies the render quality of the image. A higher number indicates better quality with more samples per pixel. Required <code>debug_breakpoint</code> boolean Determines if the scene should break and open in Blender before rendering. This feature is functional only if scene debugging is active. Optional"},{"location":"usage/job_description/config_descriptions/rgb/#important-notes","title":"Important Notes","text":"<ul> <li> <p>The <code>id</code> is essential to distinguish between different RGB outputs, especially when handling multiple configurations.</p> </li> <li> <p>While <code>samples</code> directly impact the image quality, increasing the number might also increase the rendering time.</p> </li> <li> <p>Using the <code>debug_breakpoint</code> in combination with active scene debugging can be invaluable during the development process, as it allows for real-time modifications within Blender before final rendering.</p> </li> </ul>"},{"location":"usage/job_description/config_descriptions/rgb/#example-configuration","title":"Example Configuration","text":"<p>Below is a sample configuration for the RGB Output Plugin:</p> <pre><code>syclops_output_rgb:\n  - id: MainView\n    samples: 200\n    debug_breakpoint: true\n</code></pre> <p>In this configuration, the RGB output with the identifier \"MainView\" will have a quality of 200 samples per pixel. Additionally, if the scene debugging is active, the scene will break and open in Blender before rendering.</p>"},{"location":"usage/job_description/config_descriptions/rgb/#metadata-output","title":"Metadata Output","text":"<p>Along with the output files, a <code>metadata.yaml</code> file is generated in the output folder. This file contains metadata about the keypoint output, including the output type, format, description, expected steps, sensor name, and output ID.</p>"},{"location":"usage/job_description/config_descriptions/scatter/","title":"Scatter Plugin Documentation","text":"<p>The Scatter Plugin serves the purpose of distributing instances of objects on a ground surface. Various configuration options allow for intricate control over how objects are scattered.</p>"},{"location":"usage/job_description/config_descriptions/scatter/#configuration-parameters","title":"Configuration Parameters","text":"<p>The following table describes each configuration parameter for the Scatter Plugin:</p> Parameter Type Description Required <code>name</code> string Unique identifier for the plugin instance. Required <code>models</code> array or single item 3D assets intended for scattering on the floor object. Required <code>floor_object</code> string Specifies the name of the ground surface on which the models are scattered. Required <code>max_texture_size</code> integer Specifies the texture's maximum allowable pixel size for GPU RAM conservation. Optional <code>density_map</code> image/texture evaluation A texture guiding scattering density. The texture is normalized between 0-1 and density is determined by multiplying this with <code>density_max</code> at each spatial location. Optional <code>decimate_mesh_factor</code> number (0-1) Decimation factor for mesh vertices. Lower values result in fewer vertices. Optional <code>density_max</code> number evaluation Defines the maximum number of scattered instances per square meter. Required <code>distance_min</code> number evaluation Minimum allowable distance between the origins of each scattered instance. Required <code>scale_standard_deviation</code> number evaluation Standard deviation for the size of scattered instances. Required <code>seed</code> number evaluation Sets a random seed value for scattering. Required <code>class_id</code> integer Specifies the Class ID in the ground truth output. Required <code>align_to_normal</code> boolean Dictates whether the scattered objects should align with the surface's normal. Optional <code>clumps</code> object Provides options for creating optional clusters of objects. Further detailed in the sub-table below. Optional"},{"location":"usage/job_description/config_descriptions/scatter/#clumps-configuration","title":"Clumps Configuration","text":"<p>When using the <code>clumps</code> parameter, consider the following additional configurations:</p> Parameter Type Description Required <code>ratio</code> number Ratio of clumped vs. individual scattered objects. Required <code>size</code> number Average number of plants or items per clump. Required <code>size_std</code> number Variance in the number of plants or items within a clump. Required <code>position_std</code> number Variability in meters of a plant or item's location relative to the clump's center. Required <code>scale_std</code> number Variability in the size of the plants or items in a clump. Required"},{"location":"usage/job_description/config_descriptions/scatter/#dynamic-evaluators","title":"Dynamic Evaluators","text":"<p>Most parameters, like <code>density_max</code> and <code>seed</code>, can be dynamically evaluated. This means that their values can be altered for each new frame. For more insights on dynamic evaluators and how to use them, kindly refer to Dynamic Evaluators.</p>"},{"location":"usage/job_description/config_descriptions/scatter/#example-configuration","title":"Example Configuration","text":"<pre><code>scene:\n  syclops_plugin_scatter:\n    - name: \"Forest\"\n      models: Example Assets/Trees\n      floor_object: \"Ground\"\n      max_texture_size: 2048\n      density_max: 5\n      distance_min: 0.5\n      scale_standard_deviation: 0.1\n      seed: 42\n      class_id: 3\n      align_to_normal: true\n      clumps:\n        ratio: 0.7\n        size: 5\n        size_std: 2\n        position_std: 0.25\n        scale_std: 0.05\n</code></pre> <p>The above configuration will scatter tree models across the ground surface. These trees will have a mix of clumped and individual placements, with specifics detailed by the <code>clumps</code> parameters.</p>"},{"location":"usage/job_description/config_descriptions/simulated_scatter/","title":"Simulated Scatter Plugin Documentation","text":"<p>The Simulated Scatter Plugin scatters 3D assets on a floor object and simulates physics to drop them realistically on the surface.</p>"},{"location":"usage/job_description/config_descriptions/simulated_scatter/#configuration-parameters","title":"Configuration Parameters","text":"<p>The following table describes each configuration parameter for the Simulated Scatter Plugin:</p> Parameter Type Description Requirement <code>name</code> string Unique identifier of the plugin Required <code>models</code> array or single item 3D assets to scatter. Required <code>floor_object</code> string Name of the floor object to scatter on. Required <code>max_texture_size</code> integer Maximum texture size in pixels. Will reduce the texture to save GPU RAM. Optional <code>decimate_mesh_factor</code> number (0-1) Factor between 0-1 that decimates the number of vertices of the mesh. Lower means less vertices. Optional <code>density</code> number Density of objects per square meter. Required <code>density_texture</code> image/texture evaluation Texture that alters the density per pixel. Needs to be a single channel image that is normalized to 0-1. Optional <code>scale_standard_deviation</code> number Standard deviation of the scale randomization. Required <code>convex_decomposition_quality</code> integer (1-100) Quality setting for the convex decomposition. Higher means more accurate but slower. Required <code>simulation_steps</code> integer Number of simulation steps to run. Required"},{"location":"usage/job_description/config_descriptions/simulated_scatter/#dynamic-evaluators","title":"Dynamic Evaluators","text":"<p>Parameters like <code>density_texture</code> and <code>scale_standard_deviation</code> can be dynamically evaluated. This means that their values can be altered for each new frame. For more insights on dynamic evaluators and how to use them, kindly refer to Dynamic Evaluators.</p>"},{"location":"usage/job_description/config_descriptions/simulated_scatter/#example-configuration","title":"Example Configuration","text":"<pre><code>scene:\n  syclops_plugin_simulated_scatter:  \n    - name: \"Rock Scatter\"\n      models: Example Assets/Rocks\n      floor_object: \"Ground\" \n      max_texture_size: 2048\n      density: 5\n      scale_standard_deviation: 0.3\n      convex_decomposition_quality: 90\n      simulation_steps: 100\n</code></pre> <p>The above configuration will scatter rock models across the ground surface. The rocks will be dropped from above and settle into physically realistic positions using a convex decomposition simulation. The simulation will run for 100 steps to allow the rocks to come to rest.</p>"},{"location":"usage/job_description/config_descriptions/structured_light/","title":"Structured Light Output Documentation","text":"<p>The Structured Light Output generates structured light patterns projected onto the scene, which can be used for 3D reconstruction and depth estimation.</p> <p></p>"},{"location":"usage/job_description/config_descriptions/structured_light/#configuration-parameters","title":"Configuration Parameters","text":"<p>The following table describes each configuration parameter for the Structured Light Output:</p> Parameter Type Description Requirement <code>id</code> string A unique identifier for the output. Required <code>frame_id</code> string The ID of the transformation frame to which the structured light projector is attached. Required <code>intensity</code> float The intensity of the projected light pattern. Required <code>scale</code> float The scale of the light pattern, controlling the density of the dots. Required <code>samples</code> integer The number of samples per pixel for rendering the structured light image. Higher values result in better quality but slower rendering. Required <code>debug_breakpoint</code> boolean If set to <code>true</code> and the scene debugging is active, the rendering process will pause and open Blender before proceeding. Optional"},{"location":"usage/job_description/config_descriptions/structured_light/#example-configuration","title":"Example Configuration","text":"<pre><code>syclops_output_structured_light:\n  - id: main_cam_structured_light\n    frame_id: \"camera_link\"\n    intensity: 10000\n    scale: 60\n    samples: 256\n    debug_breakpoint: True\n</code></pre> <p>In this example, a structured light output is configured with the identifier <code>main_cam_structured_light</code>. The light projector is attached to the <code>camera_link</code> transformation frame. The intensity of the projected pattern is set to 10000, and the scale is set to 60. The image will be rendered with 256 samples per pixel. If scene debugging is active, the rendering process will pause and open Blender before proceeding.</p>"},{"location":"usage/job_description/config_descriptions/structured_light/#output-format","title":"Output Format","text":"<p>The structured light output is saved as a grayscale PNG image for each frame, with the projected dot pattern visible on the scene objects. The images are stored in the <code>&lt;sensor_name&gt;_annotations/structured_light/</code> folder.</p> <p>The structured light output can be used in multiple cameras to simulate stereo vision and generate depth maps. The structured light patterns can be used for 3D reconstruction and depth estimation.</p>"}]}